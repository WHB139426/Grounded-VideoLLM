nohup: ignoring input
[2024-08-09 02:58:45,104] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-08-09 02:58:45,105] torch.distributed.run: [WARNING] 
[2024-08-09 02:58:45,105] torch.distributed.run: [WARNING] *****************************************
[2024-08-09 02:58:45,105] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-08-09 02:58:45,105] torch.distributed.run: [WARNING] *****************************************
Namespace(seed=42, model='llava_next_video', llm='llama3', dataset='mix_grounded', max_txt_len=4096, num_temporal_tokens=300, num_frames=96, num_segs=12, stage='grounded', lora=True, sharding_strategy='full-shard', epoch=1, lora_lr=0.0002, mm_proj_lr=1e-05, lr=2e-05, global_batch_size=192, per_device_batch_size=2, warmup_ratio=0.03, lr_scheduler_type='linear-warmup+cosine-decay', save_dir='/home/haibo/weights/ckpt', pretrained_proj='/home/haibo/weights/ckpt/fsdp_pretrain_llava_next_video_mix_pretrain_multi_modal_projector_video_projecter.pth')
08/09 [02:58:59] INFO     | >> [*] VLM Training :: Gathering Light   train.py:71
                 INFO     | >>     |=> "Life is like a prism; what   train.py:78
                          you see depends on how you turn the                   
                          glass."                                               
                 INFO     | >> [*] Instantiating VLM                 train.py:82
/home/haibo/workspace/miniconda3/envs/grounded-videollm/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
/home/haibo/workspace/miniconda3/envs/grounded-videollm/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
/home/haibo/workspace/miniconda3/envs/grounded-videollm/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
/home/haibo/workspace/miniconda3/envs/grounded-videollm/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
loading vision_tower
loading vision_tower
loading vision_tower
loading vision_tower
loading video_encoder
loading video_encoder
loading video_encoder
loading video_encoder
08/09 [02:59:22] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (pos_embed)                                      
08/09 [02:59:23] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (pos_embed)                                      
08/09 [02:59:23] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (clip_pos_embed)                                 
                 INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (clip_pos_embed)                                 
08/09 [02:59:23] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (pos_embed)                                      
                 INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (clip_pos_embed)                                 
08/09 [02:59:23] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (pos_embed)                                      
                 INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (clip_pos_embed)                                 
loading multi_modal_projector
loading multi_modal_projector
loading video_projector
loading multi_modal_projector
loading language_model
loading video_projector
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
loading language_model
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
loading video_projector
loading language_model
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]loading multi_modal_projector
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]loading video_projector
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]loading language_model
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.27s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.30s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:10,  1.76s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:06,  1.33s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:06,  1.33s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:06,  1.33s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:03<00:08,  1.69s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:04<00:05,  1.45s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:04<00:05,  1.44s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:04<00:05,  1.43s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:05<00:04,  1.46s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:05<00:04,  1.45s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:05<00:06,  1.66s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:05<00:04,  1.44s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:07<00:02,  1.46s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:07<00:02,  1.46s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:07<00:02,  1.45s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:06<00:04,  1.64s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:08<00:01,  1.45s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:08<00:01,  1.43s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:08<00:01,  1.42s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:08<00:03,  1.67s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:09<00:00,  1.26s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:09<00:00,  1.35s/it]
Frozen ViT
Frozen video_encoder
Loading checkpoint shards: 100%|██████████| 7/7 [00:09<00:00,  1.24s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:09<00:00,  1.34s/it]
Frozen ViT
Frozen video_encoder
Loading checkpoint shards: 100%|██████████| 7/7 [00:09<00:00,  1.24s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:09<00:00,  1.34s/it]
Frozen ViT
Frozen video_encoder
Loading checkpoint shards:  86%|████████▌ | 6/7 [00:10<00:01,  1.66s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.55s/it]
Frozen ViT
Frozen video_encoder
LORA llm
LORA llm
LORA llm
LORA llm
Frozen Part LLM
Frozen Part LLM
Frozen Part LLM
Frozen Part LLM
{'Total': 9767440301, 'Trainable': 1432344109}
08/09 [03:01:07] INFO     | >> [*] Creating Dataset                 train.py:116
08/09 [03:01:16] INFO     | >> [*] Initializing Train Strategy      train.py:162
08/09 [03:01:37] INFO     | >> [*] FSDP Full-Shard Strategy =>>      fsdp.py:261
                          Finalized Training Setup:                             
                                   |-> Global (Effective) Batch Size            
                          = 192                                                 
                                   |-> Per-Device Batch Size = 2                
                                   |-> Distributed World Size = 4               
                                   |-> Gradient Accumulation Steps =            
                          24                                                    
                                                                                
                                   |-> LLM Backbone FSDP Gradient               
                          Checkpointing = True                                  
                                   |-> Use FSDP Mixed Precision =               
                          True                                                  
                                           |-> Parameter Precision =            
                          torch.bfloat16                                        
                                           |-> Reduction Precision =            
                          torch.bfloat16                                        
                                           |-> Buffer Precision =               
                          torch.bfloat16                                        
                                                                                
                                   |-> Default AdamW LR = 2e-05                 
                                   |-> AdamW Weight Decay = 0.0                 
                                   |-> LR Scheduler Type =                      
                          linear-warmup+cosine-decay                            
                                   |-> LR Scheduler Warmup Steps                
                          (Ratio) = 52 (0.03)                                   
                                   |-> Dataset Size = 337344                    
                          Examples                                              
                                   |-> Max Steps = 1757                         
                                                                                
                 INFO     | >> [*] Starting Training Loop           train.py:186
  0%|          | 0/1756 [00:00<?, ?it/s][h264 @ 0x1308b3c0] mmco: unref short failure
[h264 @ 0x1308b3c0] mmco: unref short failure
[h264 @ 0x1308b3c0] mmco: unref short failure
[h264 @ 0x1308b3c0] mmco: unref short failure
[h264 @ 0x1308b3c0] mmco: unref short failure
[h264 @ 0x107c177c0] mmco: unref short failure
[h264 @ 0x107c177c0] mmco: unref short failure
[h264 @ 0x107c177c0] mmco: unref short failure
[h264 @ 0x107c177c0] mmco: unref short failure
[h264 @ 0x118de6540] mmco: unref short failure
[h264 @ 0x118de6540] mmco: unref short failure
[h264 @ 0x118de6540] mmco: unref short failure
[h264 @ 0x118de6540] mmco: unref short failure
[h264 @ 0x118de6540] mmco: unref short failure
[h264 @ 0x118de6540] mmco: unref short failure
[h264 @ 0x118de6540] mmco: unref short failure
[h264 @ 0x107c1d0c0] mmco: unref short failure
[h264 @ 0x107c1d0c0] mmco: unref short failure
[h264 @ 0x107c1d0c0] mmco: unref short failure
[h264 @ 0x107c1d0c0] mmco: unref short failure
[h264 @ 0x43c0eac0] mmco: unref short failure
[h264 @ 0x43c0eac0] mmco: unref short failure
[h264 @ 0x43c0eac0] mmco: unref short failure
[h264 @ 0x43c0eac0] mmco: unref short failure
[h264 @ 0x43c0eac0] mmco: unref short failure
[h264 @ 0x3d9c9fc0] mmco: unref short failure
[h264 @ 0x3d9c9fc0] mmco: unref short failure
[h264 @ 0x3d9c9fc0] mmco: unref short failure
[h264 @ 0x3d9c9fc0] mmco: unref short failure
[h264 @ 0x3d9c9fc0] mmco: unref short failure
[h264 @ 0x3d9c9fc0] mmco: unref short failure
[h264 @ 0x3d9c9fc0] mmco: unref short failure
[h264 @ 0x3d9c9fc0] mmco: unref short failure
[h264 @ 0x3d9c9fc0] mmco: unref short failure
[h264 @ 0x3d9c9fc0] mmco: unref short failure
[h264 @ 0x43c87e80] mmco: unref short failure
[h264 @ 0x43c87e80] mmco: unref short failure
[h264 @ 0x1017fcb00] mmco: unref short failure
[h264 @ 0x1017fcb00] mmco: unref short failure
[h264 @ 0x1017fcb00] mmco: unref short failure
[h264 @ 0x1017fcb00] mmco: unref short failure
[h264 @ 0x1017fcb00] mmco: unref short failure
[h264 @ 0x1017fcb00] mmco: unref short failure
[h264 @ 0x106975240] mmco: unref short failure
[h264 @ 0x106975240] mmco: unref short failure
[h264 @ 0xa9cef3c0] mmco: unref short failure
[h264 @ 0xa9cef3c0] mmco: unref short failure
[h264 @ 0x106975240] mmco: unref short failure
[h264 @ 0x106975240] mmco: unref short failure
[h264 @ 0x106975240] mmco: unref short failure
[h264 @ 0x106975240] mmco: unref short failure
[h264 @ 0x106975240] mmco: unref short failure
[h264 @ 0x106975240] mmco: unref short failure
[h264 @ 0x106975240] mmco: unref short failure
[h264 @ 0x106975240] mmco: unref short failure
[h264 @ 0xa9d38000] mmco: unref short failure
[h264 @ 0xa9d38000] mmco: unref short failure
[h264 @ 0xa9d38000] mmco: unref short failure
[h264 @ 0xa9d38000] mmco: unref short failure
  0%|          | 1/1756 [05:26<159:22:54, 326.94s/it][h264 @ 0x107c1d0c0] mmco: unref short failure
[h264 @ 0x107c1d0c0] mmco: unref short failure
[h264 @ 0x107c1d0c0] mmco: unref short failure
[h264 @ 0x107c1d0c0] mmco: unref short failure
[h264 @ 0x107c1d0c0] mmco: unref short failure
[h264 @ 0xad892900] mmco: unref short failure
[h264 @ 0x118dea740] mmco: unref short failure
[h264 @ 0x118dea740] mmco: unref short failure
[h264 @ 0x118dea740] mmco: unref short failure
[h264 @ 0x118dea740] mmco: unref short failure
[h264 @ 0x118dea740] mmco: unref short failure
[h264 @ 0xa9ccacc0] mmco: unref short failure
[h264 @ 0xa9ccacc0] mmco: unref short failure
[h264 @ 0xa9ccacc0] mmco: unref short failure
[h264 @ 0xa9ccacc0] mmco: unref short failure
[h264 @ 0xda484780] mmco: unref short failure
