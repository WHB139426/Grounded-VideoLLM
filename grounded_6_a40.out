nohup: ignoring input
[2024-08-09 00:19:56,395] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-08-09 00:19:56,395] torch.distributed.run: [WARNING] 
[2024-08-09 00:19:56,395] torch.distributed.run: [WARNING] *****************************************
[2024-08-09 00:19:56,395] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-08-09 00:19:56,395] torch.distributed.run: [WARNING] *****************************************
Namespace(seed=42, model='llava_next_video', llm='llama3', dataset='mix_grounded', max_txt_len=4096, num_temporal_tokens=300, num_frames=96, num_segs=12, stage='grounded', lora=True, sharding_strategy='full-shard', epoch=1, lora_lr=0.0002, mm_proj_lr=1e-05, lr=2e-05, global_batch_size=108, per_device_batch_size=3, warmup_ratio=0.03, lr_scheduler_type='linear-warmup+cosine-decay', save_dir='/home/haibo/weights/ckpt', pretrained_proj='/home/haibo/weights/ckpt/fsdp_pretrain_llava_next_video_mix_pretrain_multi_modal_projector_video_projecter.pth')
08/09 [00:20:06] INFO     | >> [*] VLM Training :: Gathering Light   train.py:71
                 INFO     | >>     |=> "Life is like a prism; what   train.py:78
                          you see depends on how you turn the                   
                          glass."                                               
                 INFO     | >> [*] Instantiating VLM                 train.py:82
/home/haibo/workspace/miniconda3/envs/grounded-videollm/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
/home/haibo/workspace/miniconda3/envs/grounded-videollm/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
/home/haibo/workspace/miniconda3/envs/grounded-videollm/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
/home/haibo/workspace/miniconda3/envs/grounded-videollm/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
loading vision_tower
/home/haibo/workspace/miniconda3/envs/grounded-videollm/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
loading vision_tower
loading vision_tower
/home/haibo/workspace/miniconda3/envs/grounded-videollm/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
loading vision_tower
loading vision_tower
loading vision_tower
loading video_encoder
loading video_encoder
loading video_encoder
loading video_encoder
loading video_encoder
loading video_encoder
08/09 [00:20:23] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (pos_embed)                                      
                 INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (clip_pos_embed)                                 
08/09 [00:20:24] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (pos_embed)                                      
                 INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (clip_pos_embed)                                 
08/09 [00:20:24] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (pos_embed)                                      
                 INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (clip_pos_embed)                                 
08/09 [00:20:24] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (pos_embed)                                      
                 INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (clip_pos_embed)                                 
loading multi_modal_projector
08/09 [00:20:24] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (pos_embed)                                      
08/09 [00:20:24] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (pos_embed)                                      
                 INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (clip_pos_embed)                                 
                 INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (clip_pos_embed)                                 
loading video_projector
loading multi_modal_projector
loading language_model
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
loading video_projector
loading language_model
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
loading multi_modal_projector
loading multi_modal_projector
loading multi_modal_projector
loading multi_modal_projector
loading video_projector
loading video_projector
loading video_projector
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]loading video_projector
loading language_model
loading language_model
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
loading language_model
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
loading language_model
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:08,  1.49s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:09,  1.51s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:10,  1.73s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:10,  1.75s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:10,  1.74s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:10,  1.77s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:03<00:08,  1.72s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:03<00:08,  1.78s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:03<00:09,  1.81s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:03<00:09,  1.80s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:03<00:09,  1.82s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:03<00:09,  1.83s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:05<00:07,  1.76s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:05<00:07,  1.75s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:05<00:06,  1.74s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:05<00:07,  1.75s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:05<00:07,  1.77s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:05<00:07,  1.77s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:06<00:05,  1.72s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:06<00:05,  1.68s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:06<00:04,  1.65s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:06<00:05,  1.67s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:06<00:05,  1.69s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:06<00:05,  1.70s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:08<00:03,  1.67s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:08<00:03,  1.63s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:08<00:03,  1.60s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:08<00:03,  1.61s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:08<00:03,  1.64s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:08<00:03,  1.66s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:10<00:01,  1.65s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:09<00:01,  1.63s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:09<00:01,  1.58s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:09<00:01,  1.60s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:10<00:01,  1.63s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:10<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.56s/it]
Frozen ViT
Frozen video_encoder
Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.55s/it]
Frozen ViT
Frozen video_encoder
Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.53s/it]
Frozen ViT
Frozen video_encoder
Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.53s/it]
Frozen ViT
Frozen video_encoder
Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.55s/it]
Frozen ViT
Frozen video_encoder
Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.57s/it]
Frozen ViT
Frozen video_encoder
LORA llm
LORA llm
LORA llm
LORA llm
LORA llm
LORA llm
Frozen Part LLM
Frozen Part LLM
Frozen Part LLM
Frozen Part LLM
Frozen Part LLM
Frozen Part LLM
{'Total': 9767440301, 'Trainable': 1432344109}
08/09 [00:22:10] INFO     | >> [*] Creating Dataset                 train.py:116
