nohup: ignoring input
[2024-08-09 07:04:35,388] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-08-09 07:04:35,389] torch.distributed.run: [WARNING] 
[2024-08-09 07:04:35,389] torch.distributed.run: [WARNING] *****************************************
[2024-08-09 07:04:35,389] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-08-09 07:04:35,389] torch.distributed.run: [WARNING] *****************************************
Namespace(seed=42, model='llava_next_video', llm='llama3', dataset='mix_sft', max_txt_len=2048, num_temporal_tokens=300, num_frames=96, num_segs=12, stage='sft', lora=True, sharding_strategy='full-shard', epoch=1, lora_lr=0.0002, mm_proj_lr=1e-05, lr=2e-05, global_batch_size=144, per_device_batch_size=6, warmup_ratio=0.03, lr_scheduler_type='linear-warmup+cosine-decay', save_dir='/data/hvw5451/weights/ckpt', pretrained_proj='/data/hvw5451/weights/ckpt/grounded_llava_next_video_llama3_mix_grounded_multi_modal_projector_video_projecter_language_model.pth')
08/09 [07:04:41] INFO     | >> [*] VLM Training :: Gathering Light   train.py:71
                 INFO     | >>     |=> "Life is like a prism; what   train.py:78
                          you see depends on how you turn the                   
                          glass."                                               
                 INFO     | >> [*] Instantiating VLM                 train.py:82
/data/hvw5451/miniconda3/envs/grounded-videollm/lib/python3.9/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
/data/hvw5451/miniconda3/envs/grounded-videollm/lib/python3.9/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
/data/hvw5451/miniconda3/envs/grounded-videollm/lib/python3.9/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
/data/hvw5451/miniconda3/envs/grounded-videollm/lib/python3.9/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
/data/hvw5451/miniconda3/envs/grounded-videollm/lib/python3.9/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
/data/hvw5451/miniconda3/envs/grounded-videollm/lib/python3.9/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
/data/hvw5451/miniconda3/envs/grounded-videollm/lib/python3.9/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
loading vision_tower
loading vision_tower
loading vision_tower
loading vision_tower
/data/hvw5451/miniconda3/envs/grounded-videollm/lib/python3.9/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
loading vision_tower
loading vision_tower
loading vision_tower
loading vision_tower
loading video_encoder
loading video_encoder
loading video_encoder
loading video_encoder
loading video_encoder
loading video_encoder
loading video_encoder
loading video_encoder
08/09 [07:04:59] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (pos_embed)                                      
                 INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (clip_pos_embed)                                 
08/09 [07:04:59] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (pos_embed)                                      
                 INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (clip_pos_embed)                                 
loading multi_modal_projector
08/09 [07:04:59] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (pos_embed)                                      
08/09 [07:04:59] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (pos_embed)                                      
                 INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (clip_pos_embed)                                 
08/09 [07:04:59] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (pos_embed)                                      
                 INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (clip_pos_embed)                                 
                 INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (clip_pos_embed)                                 
loading video_projector
08/09 [07:04:59] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (pos_embed)                                      
                 INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (clip_pos_embed)                                 
loading language_model
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
loading multi_modal_projector
loading multi_modal_projector
loading video_projector
loading multi_modal_projector
loading multi_modal_projector
loading multi_modal_projector
loading language_model
loading video_projector
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
08/09 [07:05:00] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (pos_embed)                                      
                 INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (clip_pos_embed)                                 
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]loading video_projector
loading video_projector
loading video_projector
loading language_model
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
loading language_model
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
loading language_model
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
loading language_model
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
loading multi_modal_projector
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]loading video_projector
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]loading language_model
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
08/09 [07:05:01] INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (pos_embed)                                      
                 INFO     | >> Temporal interpolate from 4   internvideo2.py:291
                          to 8 (clip_pos_embed)                                 
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:08,  1.42s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:06,  1.16s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:06,  1.16s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:06,  1.15s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.19s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.20s/it]loading multi_modal_projector
loading video_projector
loading language_model
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.20s/it]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:06,  1.30s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.12s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.13s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.13s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.16s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.15s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.18s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.24s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.16s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:08,  1.35s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.16s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.16s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.17s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.23s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.21s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.22s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.17s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.18s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:06,  1.30s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.17s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.18s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.26s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.23s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:06<00:02,  1.19s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.16s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.17s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.16s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:05,  1.26s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.17s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:06<00:02,  1.22s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:06<00:02,  1.22s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:07<00:01,  1.18s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:06<00:01,  1.17s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:07<00:01,  1.18s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:06<00:01,  1.17s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:05<00:03,  1.24s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:07<00:01,  1.17s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:07<00:01,  1.20s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.13s/it]
Frozen ViT
Frozen video_encoder
Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.00s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.09s/it]
Frozen ViT
Frozen video_encoder
Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.10s/it]
Frozen ViT
Frozen video_encoder
Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.00s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.09s/it]
Frozen ViT
Frozen video_encoder
Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.10s/it]
Frozen ViT
Frozen video_encoder
Loading checkpoint shards:  86%|████████▌ | 6/7 [00:07<00:01,  1.22s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.13s/it]
Frozen ViT
Frozen video_encoder
Loading checkpoint shards:  71%|███████▏  | 5/7 [00:06<00:02,  1.23s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.14s/it]
Frozen ViT
Frozen video_encoder
Loading checkpoint shards:  86%|████████▌ | 6/7 [00:07<00:01,  1.19s/it]